{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.Importing Files"
      ],
      "metadata": {
        "id": "BOyP_GVhqps4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRfZ5QUJoxGO"
      },
      "outputs": [],
      "source": [
        "# connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# looked at files in directory\n",
        "!ls /drive/MyDrive/files_coding"
      ],
      "metadata": {
        "id": "3-mdtIwCq98R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open file c_1984.txt\n",
        "f1 = open('/drive/MyDrive/files_coding/c_1984.txt')\n",
        "# store f1 in c1\n",
        "c1 = f1.read()\n",
        "# open file Frankenstein.txt\n",
        "f2 = open('/drive/MyDrive/files_coding/Frankenstein.txt')\n",
        "# store f2 in c2\n",
        "c2 = f2.read()"
      ],
      "metadata": {
        "id": "Bc3OWp7jrOiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check content 1\n",
        "c1"
      ],
      "metadata": {
        "id": "5tqF1xeCrmJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check content 2\n",
        "c2"
      ],
      "metadata": {
        "id": "gGqjl-91ruPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Tokenize"
      ],
      "metadata": {
        "id": "t_Qjo5dVr7Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk library\n",
        "import nltk\n",
        "# download lexical data base for the English language\n",
        "nltk.download('wordnet')\n",
        "# import tools for tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Fd5K8a5JsNL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize c1 and c2\n",
        "tk1 = nltk.word_tokenize(c1.lower())\n",
        "tk2 = nltk.word_tokenize(c2.lower())"
      ],
      "metadata": {
        "id": "0gqv0NT6tZy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check tk1\n",
        "tk2"
      ],
      "metadata": {
        "id": "rouGN_qEtg_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check tk2\n",
        "tk2"
      ],
      "metadata": {
        "id": "xGlmIUSBtnkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Cleaning Tokens for NLP"
      ],
      "metadata": {
        "id": "d43tAX48tt5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "# create an extended punctuation set to get rid of unwanted symbols\n",
        "#.union for other unwanted symblos that are not in ep\n",
        "ep = set(string.punctuation).union({'--', 's', \"’\", \"'s\", '“', '”'})\n",
        "\n",
        "# check which symbolas already in extended punctuation\n",
        "print(\"What is included in extended punctuation:\")\n",
        "count = 0\n",
        "for symbol in ep:\n",
        "    print(symbol, end=\" \")\n",
        "    count += 1\n",
        "    if count % 5 == 0:\n",
        "        print()"
      ],
      "metadata": {
        "id": "y0siAxKmt6aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk1wp = [w for w in tk1 if w not in ep]\n",
        "tk2wp = [w for w in tk2 if w not in ep]"
      ],
      "metadata": {
        "id": "0xC-PuHNucMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check frist tokens without punctuation\n",
        "tk1wp"
      ],
      "metadata": {
        "id": "yVfHwUWeugH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check second tokens without punctuation\n",
        "tk2wp"
      ],
      "metadata": {
        "id": "DjwDmT2LupS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "pJl_PrqsuyRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create list of tokens that are not in stopwords and extnded punctuation\n",
        "tk1wsw =[w for w in tk1wp if w not in stopwords.words('english')]\n",
        "tk2wsw =[w for w in tk2wp if w not in stopwords.words('english')]"
      ],
      "metadata": {
        "id": "DDRauFowu5UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check first cleaned tokens\n",
        "tk1wsw"
      ],
      "metadata": {
        "id": "h5DLBNVnu_4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check second cleaned tokens\n",
        "tk2wsw"
      ],
      "metadata": {
        "id": "Yc_nQbhdvGfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Preparing Tokens for analysys"
      ],
      "metadata": {
        "id": "SHpWRdz90Hqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import stemming algorythm\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "# apply stemming\n",
        "stk1 = [PorterStemmer().stem(w) for w in tk1wsw]\n",
        "stk2 = [PorterStemmer().stem(w) for w in tk2wsw]"
      ],
      "metadata": {
        "id": "vQjWKKU20KnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check stemmed tokens 1\n",
        "stk1"
      ],
      "metadata": {
        "id": "GRRxhOzi0SPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check stemmed tokens 2\n",
        "stk2"
      ],
      "metadata": {
        "id": "TawmYEDU0eVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import lemmatization algorythm\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# apply lemmatization\n",
        "tkl1 = [WordNetLemmatizer().lemmatize(w, \"n\") for w in tk1wsw]\n",
        "tkl2 = [WordNetLemmatizer().lemmatize(w, \"n\") for w in tk2wsw]"
      ],
      "metadata": {
        "id": "lVVs3B4_0gvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check lammatized tokens 1\n",
        "tkl1"
      ],
      "metadata": {
        "id": "eQAO9Vn40rdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check lemmatized tokens 2\n",
        "tkl2"
      ],
      "metadata": {
        "id": "RC6wESqC1rzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Analyze Frequency of words"
      ],
      "metadata": {
        "id": "C11Nqvlh4kI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import counter from collections\n",
        "from collections import Counter\n",
        "# count the mmost common tokens\n",
        "cnt1 = Counter(stk1).most_common(10)\n",
        "cnt2 = Counter(stk2).most_common(10)"
      ],
      "metadata": {
        "id": "ihmGlcjH4sAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check most common tokens 1\n",
        "cnt1"
      ],
      "metadata": {
        "id": "jSufOsOg5GFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check most common tokens 2\n",
        "cnt2"
      ],
      "metadata": {
        "id": "QzlQRNwf5HQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels1, values1 = zip(*cnt1)\n",
        "labels2, values2 = zip(*cnt2)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(labels1, values1, color='blue')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Most common stemmed tokens in 1984')\n",
        "# rotation for easier viewing\n",
        "plt.xticks(rotation=45)\n",
        "# numerate values\n",
        "for i, v in enumerate(values1):\n",
        "    plt.text(i, v + 0.1, str(v), ha='center')\n",
        "# show the plot\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(labels2, values2, color='red')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Most common stemmed tokens in Frankenstein')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "\n",
        "for i, v in enumerate(values2):\n",
        "    plt.text(i, v + 0.1, str(v), ha='center')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TM1k1GDh51hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same plot for lemmatized tokens for comparison"
      ],
      "metadata": {
        "id": "JRB4I4868Rrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnt1_lemmatized = Counter(stk1).most_common(10)\n",
        "cnt2_lemmatized = Counter(stk2).most_common(10)"
      ],
      "metadata": {
        "id": "N9Drt9IZ656w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels1, values1 = zip(*cnt1_lemmatized)\n",
        "labels2, values2 = zip(*cnt2_lemmatized)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(labels1, values1, color='blue')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Most common lemmatized tokens in 1984')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "for i, v in enumerate(values1):\n",
        "    plt.text(i, v + 0.1, str(v), ha='center')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(labels2, values2, color='red')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Most common lemmatized tokens in Frankenstein')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "\n",
        "for i, v in enumerate(values2):\n",
        "    plt.text(i, v + 0.1, str(v), ha='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uzAE9XYb7LeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Analysis - Similarities Versus"
      ],
      "metadata": {
        "id": "f84LhVXN-H1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version A"
      ],
      "metadata": {
        "id": "EiExdB2MCnBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages for similarity analysys\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "G5kbbLC4-M6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books = [tk1wsw, tk2wsw]\n",
        "# model for calculation\n",
        "model = Word2Vec(books, min_count=1)\n",
        "# create vectors\n",
        "book1_vector = model.wv[tk1wsw]\n",
        "book2_vector = model.wv[tk2wsw]\n",
        "\n",
        "# identify similarities\n",
        "# check overall similarity\n",
        "similarity_scores = cosine_similarity(book1_vector, book2_vector)\n",
        "print(\"Similarity_scores:\")\n",
        "print(similarity_scores)\n",
        "overall_similarity = np.mean(similarity_scores)\n",
        "print(\"Overall similarity: \", overall_similarity)"
      ],
      "metadata": {
        "id": "MSH7JlBB-Q-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27fedf95-715d-4dbb-8cb4-fb91725fb2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity_scores:\n",
            "[[ 0.20701683  0.08079749 -0.1707272  ...  0.02707134 -0.11814065\n",
            "  -0.09474735]\n",
            " [ 0.02318923 -0.02310109 -0.06204247 ...  0.19780672  0.01102354\n",
            "  -0.0352779 ]\n",
            " [ 0.21562698  0.01395856  0.03951131 ...  0.07064446  0.09430897\n",
            "   0.10130158]\n",
            " ...\n",
            " [-0.02813942 -0.02208235 -0.11588963 ... -0.04895997  0.22399652\n",
            "  -0.14175871]\n",
            " [ 0.09384546  0.03260145 -0.00974998 ...  0.026046    0.02924124\n",
            "   0.1071071 ]\n",
            " [ 0.07910256  0.04661493  0.08799782 ... -0.11732103 -0.11337622\n",
            "   0.01238563]]\n",
            "Overall similarity:  0.0010516167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "w5tnyCG5BEkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform Singular Value Decomposition\n",
        "svd = TruncatedSVD(n_components=2)\n",
        "reduced_similarity = svd.fit_transform(similarity_scores)"
      ],
      "metadata": {
        "id": "KjIpHFlpBJdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale the reduced similarity scores to therange [-1, 1]\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "reduced_similarity_scaled = scaler.fit_transform(reduced_similarity)\n",
        "# check the reduced similarity scores\n",
        "print(\"Reduced Similarity Scores (2 dimensions):\")\n",
        "print(reduced_similarity_scaled)\n"
      ],
      "metadata": {
        "id": "vqymjKC7BMKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract x and y coordinates\n",
        "x = [row[0] for row in reduced_similarity_scaled]\n",
        "y = [row[1] for row in reduced_similarity_scaled]"
      ],
      "metadata": {
        "id": "AHIixcZhBipV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(x, y, color='blue')\n",
        "plt.title('Reduced Similarity Scores Visualization')\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QS-rrEUmBkwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version B"
      ],
      "metadata": {
        "id": "d60xmYWBAo-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages from sklearn for similarity analysis\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "xSwkxRbNAq6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in this version use raw content data before tokenization\n",
        "texts = [c1, c2]\n",
        "\n",
        "# preprocess text with lower()\n",
        "preprocessed_text = [text.lower() for text in texts]\n",
        "\n",
        "# transform the text into a vector\n",
        "# creating Bag-of-Words\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(preprocessed_text)\n",
        "# calculate cosine similarity\n",
        "cosine_sim = cosine_similarity(X)"
      ],
      "metadata": {
        "id": "rL3GmrfsA0-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim"
      ],
      "metadata": {
        "id": "lxAGy2okBJ4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "\n",
        "# Improve the color palette\n",
        "custom_palette = sns.light_palette((210, 90, 60), input=\"husl\")\n",
        "\n",
        "# Create a heatmap with shadow and borders\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cosine_sim, annot=True, cmap=custom_palette,\n",
        "            linewidths=1, linecolor='grey', fmt=\".2f\",\n",
        "            annot_kws={\"size\": 14})\n",
        "\n",
        "# Add a gradient background\n",
        "plt.gca().patch.set_facecolor('lightgrey')\n",
        "\n",
        "# Add a title with animation\n",
        "plt.text(0.5, 1.05, 'Cosine Similarity between 1984 and Frankenstein', size=20, ha='center', transform=plt.gca().transAxes)\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "sXEQIGVIBM_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Sentiment Analysys"
      ],
      "metadata": {
        "id": "dwJ9oSDZGnAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "pNzhb_m5GuN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to analyse sentiment\n",
        "def analyze_sentiment(txt):\n",
        "  polarity = TextBlob(txt).polarity\n",
        "  if polarity > 0:\n",
        "    sentiment_label = \"positive\"\n",
        "  elif polarity < 0:\n",
        "    sentiment_label = \"negative\"\n",
        "  else:\n",
        "    sentiment_label = \"neutral\"\n",
        "\n",
        "  return sentiment_label\n"
      ],
      "metadata": {
        "id": "NkLyOHpWG5jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at sentiments\n",
        "for text in tkl1:\n",
        "  sentiment = analyze_sentiment(text)\n",
        "  print(f\"Text: '{text}' | Sentiment : '{sentiment}'\" )"
      ],
      "metadata": {
        "id": "1-EC5pWULs8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize counters for sentiment labels\n",
        "sentiment_counts = {\"positive\": 0, \"negative\": 0, \"neutral\": 0}\n"
      ],
      "metadata": {
        "id": "ndY4fmG2HzaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse sentiment for each token 1\n",
        "for text in tkl1:\n",
        "    sentiment1 = analyze_sentiment(text)\n",
        "    sentiment_counts[sentiment1] += 1"
      ],
      "metadata": {
        "id": "-bOR6SCnH6la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize\n",
        "labels = sentiment_counts.keys()\n",
        "counts = sentiment_counts.values()\n",
        "plt.pie(counts, labels=labels, colors=['green', 'red', 'gray'], autopct='%1.1f%%')\n",
        "plt.title('Sentiment Analysis Results for 1984')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LjsNYFUKNIrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse sentiment for each text 1\n",
        "for text in tkl2:\n",
        "    sentiment2 = analyze_sentiment(text)\n",
        "    sentiment_counts[sentiment2] += 1"
      ],
      "metadata": {
        "id": "kA9JBAdvNgqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize\n",
        "labels = sentiment_counts.keys()\n",
        "counts = sentiment_counts.values()\n",
        "plt.pie(counts, labels=labels, colors=['green', 'red', 'gray'], autopct='%1.1f%%')\n",
        "plt.title('Sentiment Analysis Results for Frankenstein')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZNjT5mTUNhjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Named Entity Recognition Analysys"
      ],
      "metadata": {
        "id": "oc9qLsCbOo5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Process the text with the loaded model\n",
        "doc = nlp(c1)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "d1BaUHLrPDMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the text\n",
        "from spacy import displacy\n",
        "from IPython.core.display import display, HTML\n",
        "html = displacy.render(doc, style =\"ent\", page=False)\n",
        "display(HTML(html))\n"
      ],
      "metadata": {
        "id": "7a7dZXOcPxwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "UpKGmHeiQct7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store named entity counts\n",
        "label_counts = {}\n",
        "# Iterate over the named entities and count the labels\n",
        "for ent in doc.ents:\n",
        "  label = ent.label_\n",
        "  label_counts[label] = label_counts.get(label, 0) + 1\n",
        "df = pd.DataFrame(list(label_counts.items()), columns=['Named Entity Label', 'Count'])\n",
        "print(\"1984\", df)"
      ],
      "metadata": {
        "id": "FnO1HFc7QheL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same code for c2\n"
      ],
      "metadata": {
        "id": "_AHCLESQHgfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(c2)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "1D1eOj2Sf-0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "from IPython.core.display import display, HTML\n",
        "html = displacy.render(doc, style =\"ent\", page=False)\n",
        "display(HTML(html))"
      ],
      "metadata": {
        "id": "RpcER3kAgC4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = {}\n",
        "for ent in doc.ents:\n",
        "  label = ent.label_\n",
        "  label_counts[label] = label_counts.get(label, 0) + 1\n",
        "df = pd.DataFrame(list(label_counts.items()), columns=['Named Entity Label', 'Count'])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "qZrHsVhRgFSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Sentences analysis"
      ],
      "metadata": {
        "id": "oHt1X06V3WRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "books = {'1984': c1, 'Frankenstein': c2}\n",
        "\n",
        "# Iterate over the content of each file\n",
        "for book_name, text in books.items():\n",
        "    # Process the text with the loaded model\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Initialize a list to store the lengths of sentences\n",
        "    sentence_lengths = []\n",
        "\n",
        "    # Iterate over the sentences in the document\n",
        "    for sent in doc.sents:\n",
        "        # Append the length of each sentence to the list\n",
        "        sentence_lengths.append(len(sent))\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"\\n{book_name}:\")\n",
        "    print(f\"Total number of sentences: {len(sentence_lengths)}\")\n",
        "    print(f\"Average sentence length: {sum(sentence_lengths) / len(sentence_lengths)}\")\n",
        "    print(f\"Maximum sentence length: {max(sentence_lengths)}\")\n",
        "    print(f\"Minimum sentence length: {min(sentence_lengths)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "aJICwDxl1knA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}